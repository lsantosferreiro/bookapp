{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EA3 - Ejercicio 1 - GPU.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zw-Vno_15t-E"
      },
      "source": [
        "# 1 Introducción\n",
        "\n",
        "En el siguiente ejercicio se realiza el intercambio de los elementos de un vector a otro de igual dimensión, cada elemento se reubica en la misma posición en la que se encontraba.\n",
        "\n",
        "El algoritmo está basado en la función Swap [3] de la biblioteca BLAS [2].\n",
        "Se busca comparar y analizar la performance del procesamiento de estructuras de una dimensión en GPU y en CPU.\n",
        "\n",
        "Para su implementación en GPU se desarrolló una función kernel que recibe tres parámetros: La cantidad de elementos de los vectores, el puntero al vector X y el puntero al vector Y. Gracias a ésta, el algoritmo utiliza hilos para realizar los intercambios de una manera más eficiente que si se fuera implementado en CPU.\n",
        "\n",
        "Para su implementación en CPU no se necesitó más que desarrollar un simple for con el respectivo intercambio de elementos de los vectores. A diferencia de su implementación en GPU, no son necesarias ejecuciones previas del armado del ambiente.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cRnhv_7N4Pa"
      },
      "source": [
        "---\n",
        "# 2 Armado del ambiente\n",
        "Instala en el cuaderno el módulo CUDA de Python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z74FNbCszDmw"
      },
      "source": [
        "!pip install pycuda"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzQaWRTtc1Zj"
      },
      "source": [
        "---\n",
        "# 3 Desarrollo\n",
        "Ejecución del algoritmo en GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c7mZSnu0M3m"
      },
      "source": [
        "#@title ## 3.1 Parámetros de ejecución\n",
        "#@markdown ---\n",
        "#@markdown ### Especifique la cantidad de elementos de los vectores:\n",
        "cantidad_N =   10000#@param {type: \"number\"}\n",
        "#@markdown ---\n",
        "\n",
        "#Valida parámetro\n",
        "if cantidad_N <= 0:\n",
        "  print('La cantidad de elemtentos debe ser mayor a cero.')\n",
        "else:\n",
        "  from datetime import datetime\n",
        "  from pycuda.compiler import SourceModule\n",
        "  import pycuda.driver as cuda\n",
        "  import pycuda.autoinit\n",
        "  import numpy\n",
        "\n",
        "  #Inicia tiempo total de ejecución\n",
        "  tiempo_total = datetime.now()\n",
        "\n",
        "  #Definición de función que transforma el tiempo en  milisegundos \n",
        "  tiempo_en_ms = lambda dt:(dt.days * 24 * 60 * 60 + dt.seconds) * 1000 + dt.microseconds / 1000.0\n",
        "\n",
        "  try:\n",
        "    #Define la memoria de los vectores en CPU y los setea con números\n",
        "    #enteros aleatorios entre el 1 y el 9\n",
        "    x_cpu = numpy.random.randint(1,10, cantidad_N)\n",
        "    x_cpu = x_cpu.astype(numpy.int64())\n",
        "    y_cpu = numpy.random.randint(1,10, cantidad_N)\n",
        "    y_cpu = y_cpu.astype(numpy.int64())\n",
        "  except:\n",
        "    print('Error al definir en memoria los vectores de CPU.')\n",
        "\n",
        "  #Muestra los vectores iniciales X e Y\n",
        "  print(\"Vector X inicial:\", x_cpu)\n",
        "  print(\"Vector Y inicial:\", y_cpu)\n",
        "\n",
        "  try:\n",
        "    #Reservo la memoria GPU.\n",
        "    x_gpu = cuda.mem_alloc( x_cpu.nbytes )\n",
        "    y_gpu = cuda.mem_alloc( y_cpu.nbytes )\n",
        "  except:\n",
        "    print('Error al reservar la memoria GPU.')\n",
        "\n",
        "  try:\n",
        "    #Copia la memoria al GPU.\n",
        "    cuda.memcpy_htod( x_gpu, x_cpu )\n",
        "    cuda.memcpy_htod( y_gpu, y_cpu )\n",
        "  except:\n",
        "    print('Error al copiar la memoria al GPU.')\n",
        "\n",
        "  #Define la función kernel que ejecutará en GPU.\n",
        "  module = SourceModule(\"\"\"\n",
        "  __global__ void kernel_swap( int n, int *X, int *Y )\n",
        "  {\n",
        "    int idx = threadIdx.x + blockIdx.x*blockDim.x;\n",
        "    int aux;\n",
        "    if( idx < n )\n",
        "    {\n",
        "      aux = X[idx];\n",
        "      X[idx] = Y[idx];\n",
        "      Y[idx] = aux;\n",
        "    }\n",
        "  }\n",
        "  \"\"\")\n",
        "\n",
        "  #Genera la función kernel\n",
        "  kernel = module.get_function(\"kernel_swap\")\n",
        "\n",
        "  #Define dimensiones de hilos y bloques\n",
        "  dim_hilo = 256\n",
        "  dim_bloque = numpy.int((cantidad_N+dim_hilo-1)/dim_hilo)\n",
        "\n",
        "  #Inicia el tiempo de GPU\n",
        "  tiempo_gpu = datetime.now()\n",
        "\n",
        "  #Llama a la función kernel\n",
        "  kernel(numpy.int64(cantidad_N), x_gpu, y_gpu, block=(dim_hilo, 1, 1),grid=(dim_bloque, 1,1))\n",
        "\n",
        "  #Finaliza el tiempo de GPU\n",
        "  tiempo_gpu = datetime.now() - tiempo_gpu\n",
        "\n",
        "  try:\n",
        "    #Copia el resultado desde la memoria GPU a CPU\n",
        "    cuda.memcpy_dtoh(y_cpu, y_gpu)\n",
        "    cuda.memcpy_dtoh(x_cpu, x_gpu)\n",
        "  except:\n",
        "    print('Error al copiar el resultado desde la memoria GPU a CPU.')\n",
        "\n",
        "  #Calcula tiempo total de ejecución\n",
        "  tiempo_total = datetime.now() - tiempo_total\n",
        "\n",
        "  #Muestra los vectores finales X e Y\n",
        "  print(\"Vector X final:\", x_cpu)\n",
        "  print(\"Vector Y final:\", y_cpu)\n",
        "\n",
        "  #Muestra cantidad de elementos\n",
        "  print(\"Cantidad de elementos: \", cantidad_N)\n",
        "\n",
        "  #Muestra dimensiones de hilos y bloques\n",
        "  print(\"Thread x: \", dim_hilo, \", Bloque x:\", dim_bloque)\n",
        "\n",
        "  #Muestra los tiempos de GPU y total de ejecución\n",
        "  print(\"Tiempo TOTAL: \", tiempo_en_ms(tiempo_total), \"[ms]\")\n",
        "  print(\"Tiempo GPU  : \", tiempo_en_ms( tiempo_gpu), \"[ms]\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EALIlyyG6iqP"
      },
      "source": [
        "---\n",
        "# 4 Tabla de pasos de ejecución del programa\n",
        "\n",
        "\n",
        "Procesador  |Función               |Detalle\n",
        "------------|----------------------|-----------------------------------------\n",
        "CPU         |pip install pycuda    |Instala en el cuaderno los driver de CUDA para Python.\n",
        "CPU         |@param                |Lectura del tamaño de vectores desde Colab.\n",
        "CPU         |if                    |Valida parámetro.\n",
        "CPU         |import                |Importa los módulos para funcionar.\n",
        "CPU         |datetime.now()        |Toma el tiempo actual.\n",
        "CPU         |numpy.random.randint(..)|Inicializa los vectores X e Y con enteros aleatorios entre 1 y 9.\n",
        "CPU         |astype(..)            |Castea los vectores a int64.\n",
        "CPU         |print(..)             |Muestra los vectores iniciales X e Y.\n",
        "**GPU**     |cuda.mem_alloc(..)    |Reserva la memoria en GPU.\n",
        "**GPU**     |cuda.memcpy_htod(..)  |Copia las memorias desde el CPU al GPU.\n",
        "CPU         |SourceModule(..)      |Define el código del kernel.\n",
        "CPU         |module.get_function(..)|Genera la función del kernel GPU\n",
        "CPU         |dim_tx/dim_bx         |Calcula las dimensiones.\n",
        "CPU         |datetime.now()        |Toma el tiempo actual para calcular el tiempo de GPU.\n",
        "**GPU**     |kernel(..)            |Ejecuta el kernel en GPU.\n",
        "CPU         |datetime.now()        |Toma el tiempo actual para calcular el tiempo de GPU.\n",
        "CPU         |cuda.memcpy_dtoh(..)  |Copia el resultado desde GPU memoria X a CPU memoria X y GPU memoria X a CPU memoria Y.\n",
        "CPU         |datetime.now()        |Toma el tiempo actual para calcular el tiempo total de ejecución.\n",
        "CPU         |print(..)             |Muestra los vectores finales X e Y.\n",
        "CPU         |print(..)             |Muestra la cantidad de elementos.\n",
        "CPU         |print(..)             |Muestra dimensiones de hilos y bloques.\n",
        "CPU         |print(..)             |Informa el total de ejecución.\n",
        "CPU         |print(..)             |Informa el tiempo de GPU.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzgZkrQD-UTy"
      },
      "source": [
        "---\n",
        "# 5 Conclusiones\n",
        "\n",
        "Como una conclusión general y empírica se puede observar que los datos, tras cierta cantidad de ejecuciones, arrojan que el procesamiento a partir de la GPU es muchísimo más eficiente. \n",
        "\n",
        "Por ejemplo: \n",
        "\n",
        "Ejecución de intercambio de vectores (Con 10000 elementos) en CPU: 11.987 [ms].\n",
        "\n",
        "Ejecución de intercambio de vectores (Con 10000 elementos) en GPU: 0.163 [ms].\n",
        "\n",
        "Se puede ver claramente la diferencia de tiempos entre cada ejecución al momento de realizar los intercambios.\n",
        "\n",
        "Se puede apreciar que si el vector es de un solo elemento no tiene sentido procesarlo con GPU. \n",
        "\n",
        "Ejecución de intercambio de vectores (Con 1 elemento) en CPU: 0.089 [ms].\n",
        "\n",
        "Ejecución de intercambio de vectores (Con 1 elemento) en GPU: 0.109 [ms].\n",
        "\n",
        "Esto se debe a que como el hilo hace el context switch manejando su estructura de control, se pierde más tiempo que trabajar el intercambio de manera secuencial.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hn6HOCYEjyY"
      },
      "source": [
        "---\n",
        "# 6 Bibliografía\n",
        "\n",
        "[1] Introducción a Python: [Página Colab](https://github.com/wvaliente/SOA_HPC/blob/main/Documentos/Python_Basico.ipynb) \n",
        "\n",
        "[2] Función Swap de biblioteca BLAS: [Referencia](https://software.intel.com/content/www/us/en/develop/documentation/mkl-developer-reference-c/top/blas-and-sparse-blas-routines/blas-routines/blas-level-1-routines-and-functions/cblas-swap.html)\n",
        "\n",
        "[3] Biblioteca BLAS: [Referencia](http://www.netlib.org/blas/)\n",
        "\n",
        "[4] Documentación PyCUDA: [WEB](https://documen.tician.de/pycuda/index.html)\n",
        "\n",
        "[5] Repositorio de PyCUDA: [WEB](https://pypi.python.org/pypi/pycuda)\n",
        "\n",
        "[6] ¿QUÉ ES LA COMPUTACIÓN ACELERADA POR GPU?: [Página Nvidia](https://www.nvidia.com/es-la/drivers/what-is-gpu-computing/)\n",
        "\n",
        "[7] Tutorial Point Colab: [PDF](https://github.com/wvaliente/SOA_HPC/blob/main/Documentos/markdown-cheatsheet-online.pdf)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}